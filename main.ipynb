{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Learning Procedure in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[Yoo et al. 2019] Learning Loss for Active Learning (https://arxiv.org/abs/1905.03677)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Python\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Torchvison\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom\n",
    "import models.resnet as resnet\n",
    "import models.lossnet as lossnet\n",
    "from config import *\n",
    "from data.sampler import SubsetSequentialSampler\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Seed\n",
    "random.seed(\"Wonsuk Kim\")\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "##\n",
    "# Data\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(size=32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "\n",
    "cifar10_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)\n",
    "cifar10_unlabeled   = CIFAR10('../cifar10', train=True, download=True, transform=test_transform)\n",
    "cifar10_test  = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train Utils\n",
    "iters = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis=None, plot_data=None):\n",
    "    models['backbone'].train()\n",
    "    models['module'].train()\n",
    "    global iters\n",
    "\n",
    "    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n",
    "        inputs = data[0].cuda()\n",
    "        labels = data[1].cuda()\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "        optimizers['module'].zero_grad()\n",
    "\n",
    "        scores, features = models['backbone'](inputs)\n",
    "        target_loss = criterion(scores, labels)\n",
    "\n",
    "        if epoch > epoch_loss:\n",
    "            # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n",
    "            features[0] = features[0].detach()\n",
    "            features[1] = features[1].detach()\n",
    "            features[2] = features[2].detach()\n",
    "            features[3] = features[3].detach()\n",
    "        pred_loss = models['module'](features)\n",
    "        pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n",
    "        loss            = m_backbone_loss + WEIGHT * m_module_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizers['backbone'].step()\n",
    "        optimizers['module'].step()\n",
    "\n",
    "        # Visualize\n",
    "        if (iters % 100 == 0) and (vis != None) and (plot_data != None):\n",
    "            plot_data['X'].append(iters)\n",
    "            plot_data['Y'].append([\n",
    "                m_backbone_loss.item(),\n",
    "                m_module_loss.item(),\n",
    "                loss.item()\n",
    "            ])\n",
    "            # vis.line(\n",
    "            #     X=np.stack([np.array(plot_data['X'])] * len(plot_data['legend']), 1),\n",
    "            #     Y=np.array(plot_data['Y']),\n",
    "            #     opts={\n",
    "            #         'title': 'Loss over Time',\n",
    "            #         'legend': plot_data['legend'],\n",
    "            #         'xlabel': 'Iterations',\n",
    "            #         'ylabel': 'Loss',\n",
    "            #         'width': 1200,\n",
    "            #         'height': 390,\n",
    "            #     },\n",
    "            #     win=1\n",
    "            # )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(models, dataloaders, mode='val'):\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in dataloaders[mode]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            scores, _ = models['backbone'](inputs)\n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, vis, plot_data):\n",
    "    print('>> Train a Model.')\n",
    "    best_acc = 0.\n",
    "    checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        schedulers['backbone'].step()\n",
    "        schedulers['module'].step()\n",
    "\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis, plot_data)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        if False and epoch % 5 == 4:\n",
    "            acc = test(models, dataloaders, 'test')\n",
    "            if best_acc < acc:\n",
    "                best_acc = acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                    'state_dict_module': models['module'].state_dict()\n",
    "                },\n",
    "                '%s/active_resnet18_cifar10.pth' % (checkpoint_dir))\n",
    "            print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\n",
    "    print('>> Finished.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_uncertainty(models, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "\n",
    "            scores, features = models['backbone'](inputs)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "    \n",
    "    return uncertainty.cpu()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    # vis = visdom.Visdom(server='http://localhost', port=9000)\n",
    "    plot_data = {'X': [], 'Y': [], 'legend': ['Backbone Loss', 'Module Loss', 'Total Loss']}\n",
    "\n",
    "    for trial in range(TRIALS):\n",
    "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "        indices = list(range(NUM_TRAIN))\n",
    "        random.shuffle(indices)\n",
    "        labeled_set = indices[:ADDENDUM]\n",
    "        unlabeled_set = indices[ADDENDUM:]\n",
    "        \n",
    "        train_loader = DataLoader(cifar10_train, batch_size=BATCH, \n",
    "                                  sampler=SubsetRandomSampler(labeled_set), \n",
    "                                  pin_memory=True)\n",
    "        test_loader  = DataLoader(cifar10_test, batch_size=BATCH)\n",
    "        dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "        \n",
    "        # Model\n",
    "        resnet18    = resnet.ResNet18(num_classes=10).to(device)\n",
    "        loss_module = lossnet.LossNet().to(device)\n",
    "        models      = {'backbone': resnet18, 'module': loss_module}\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Active learning cycles\n",
    "        for cycle in range(CYCLES):\n",
    "            # Loss, criterion and scheduler (re)initialization\n",
    "            criterion      = nn.CrossEntropyLoss(reduction='none')\n",
    "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n",
    "                                    momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "            optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n",
    "                                    momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
    "            sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=MILESTONES)\n",
    "\n",
    "            optimizers = {'backbone': optim_backbone, 'module': optim_module}\n",
    "            schedulers = {'backbone': sched_backbone, 'module': sched_module}\n",
    "\n",
    "            # Training and test\n",
    "            train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, vis, plot_data)\n",
    "            acc = test(models, dataloaders, mode='test')\n",
    "            print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "\n",
    "            ##\n",
    "            #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            random.shuffle(unlabeled_set)\n",
    "            subset = unlabeled_set[:SUBSET]\n",
    "\n",
    "            # Create unlabeled dataloader for the unlabeled subset\n",
    "            unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH, \n",
    "                                          sampler=SubsetSequentialSampler(subset), # more convenient if we maintain the order of subset\n",
    "                                          pin_memory=True)\n",
    "\n",
    "            # Measure uncertainty of each data points in the subset\n",
    "            uncertainty = get_uncertainty(models, unlabeled_loader)\n",
    "\n",
    "            # Index in ascending order\n",
    "            arg = np.argsort(uncertainty)\n",
    "            \n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n",
    "            unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "            # Create a new dataloader for the updated labeled dataset\n",
    "            dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH, \n",
    "                                              sampler=SubsetRandomSampler(labeled_set), \n",
    "                                              pin_memory=True)\n",
    "        \n",
    "        # Save a checkpoint\n",
    "        torch.save({\n",
    "                    'trial': trial + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                    'state_dict_module': models['module'].state_dict()\n",
    "                },\n",
    "                './cifar10/train/weights/active_resnet18_cifar10_trial{}.pth'.format(trial))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}